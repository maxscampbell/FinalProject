---
title: "Diabetes Health Indicators - Modeling"
author: "Max Campbell"
format: html
editor: visual
---



# Modeling our Data



```{r}
#| echo: false
#| warning: false

#Read in required packages
library(tidymodels)
library(caret)

#Rerun code from the EDA section to ensure all objects are visible
# Read in dataset
raw_data <- read.csv(".\\diabetes.csv", header = TRUE)

# Convert most of these into factors will sensible levels
diabetes <- raw_data |>
  mutate(
    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c("No", "Yes")),
    HighBP = factor(HighBP, levels = c(0, 1), labels = c("Low", "High")),
    HighChol = factor(HighChol, levels = c(0, 1), labels = c("Low", "High")),
    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c("No", "Yes")),
    Smoker = factor(Smoker, levels = c(0, 1), labels = c("No", "Yes")),
    Stroke = factor(Stroke, levels = c(0, 1), labels = c("No", "Yes")),
    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c("No", "Yes")),
    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c("No", "Yes")),
    Fruits = factor(Fruits, levels = c(0, 1), labels = c("No", "Yes")),
    Veggies = factor(Veggies, levels = c(0, 1), labels = c("No", "Yes")),
    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c("No", "Yes")),
    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c("No", "Yes")),
    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c("No", "Yes")),
    GenHlth = factor(GenHlth, levels = c(1:5)),
    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c("No", "Yes")),
    Sex = factor(Sex, levels = c(0, 1), labels = c("Female", "Male")),
    Age = factor(Age, levels = c(1:13), labels = c(
      "18-24", "25-29", "30-34", "35-39", "40-44",
      "45-49", "50-54", "55-59", "60-64", "65-69",
      "70-74", "75-79", "80<="
    )),
    Education = factor(Education, levels = c(1:6), labels = c(
      "Kindergarten", "Grade 8", "Grade 11",
      "GED", "Some College", "College 4+ years"
    )),
    Income = factor(Income, levels = c(1:8), labels = c(
      "< $10k", "< $15k", "< $20k", "< $25k",
      "< 35k", "< $50k", "< $75k", "More than $75k"
    )))
```



## Introduction

When looking at data like the diabetes health indicators, the first inclination is most likely to try and predict whether a person has diabetes, given the presence (or absence) of these indicators. In order to accomplish this, there are three major options that come to mind: logistic regressions, classification trees, and random forests.

A logistic regression is particularly useful in scenarios where the variable being modeled upon has two factor levels as the response variable will always output a value between 0 and 1, no matter what the indicators are. As a result, the response essentially functions as a probability value that the response's conditions are met. The way this is accomplished is by fitting the probability that the response's conditions are met (hereafter referred to as a "success") as a function of the log-odds of success.

$$
log(\frac{P(Success|x)}{1 - P(Success|x)}) = \beta_0 + \beta_1 x + ...
$$

We use this function in particular because the log-odds of success adopts a linear relationship with the parameters. The end goal of this model would be to quantify the odds of having diabetes given a set of indicators.

A second way we could approach this problem is via classification. This works because we are essentially trying to predict which group an observation is most likely to fall into (presence of diabetes, in this case), so we don't really care about the exact probability, which is what a logistic regression would seek to accomplish. Classification trees work by splitting the data into different regions based on predictors in a way that minimizes the error rate in our prediction. Then, we can give the tree an observation, and it will make decisions based on what conditions the observation meets (in this example, say that the first condition is that the individual has high cholesterol) to sift it through various "bins" of our data. Eventually, the tree will run out of decisions to make, and it will take the most prevalent response within the training data of the final bin and assign an outcome to that observation. We also have the advantage of accounting for complexity in our tree by assigning a parameter that minimizes the error rate of our predictions.

The final way we will consider in this exercise is a random forest. This is a type of ensemble tree method where we bootstrap a large number of individual classification trees in hopes of getting a more accurate prediction. The random forest method works similarly to a classification tree, with the key difference being that a subset of the predictors present in the data are randomly chosen (the exact number of which can be tuned) and subset from the data to obtain a prediction. By taking a random subset of predictors, we reduce the chances that one really strong predictor overtakes the rest of the model and basically becomes the only predictor that it uses to choose an outcome. From there, we iterate on this process numerous times, selecting a different random subset of predictors on each iteration. Once we have completed the bootstrap, we can then "average" our predictions by fitting our data to each tree within the forest and taking the most prevalent outcome per observation. This technique is advantageous because we now gain access to a measure of variability within our bootstrap iterations, so we can see how often our model is deviating from the most prevalent outcome. This is a luxury that is not afforded by a single classification tree.



```{r}
#Set a random seed for reproducible results
set.seed(72625)
# Split the dataset into a training set and a test set
diabetes_split <- initial_split(diabetes, prop = 0.7)
diabetes_train <- training(diabetes_split)
diabetes_test <- testing(diabetes_split)
diabetes_folds <- vfold_cv(diabetes_train, 5)
```



What we just did in the code shown above is split up our initial data into a training split and test split (70%/30%, respectively), and then further split the training set into five cross-validation folds. This will allow us to fit models in a way that allows us to see how much variability our predictions have, which can further optimize our model. We can then take each model and fit on the test set as a way of comparing our models!

Now that the process has been outlined, let's get into the specifics of each model type:

## Logistic Regression

In our efforts to fit a logistic regression model, we will examine three separate models to fit on the `Diabetes_binary` variable:

-   Model 1: the three predictors discussed in the EDA section (high cholesterol, BMI, and income level)

-   Model 2: every predictor explicitly related to individual health (high blood pressure, high cholesterol, BMI, history of strokes and/or heart attacks, etc., 11 total).

-   Model 3: every predictor provided in the model (21 total).

We've already established that the predictors in Model 1 are likely to significantly drive the model, at the risk of not fitting enough variables to the model. In Model 2, we are trying to see if the health indicators offer a holistic prediction, at the risk of losing valuable information from predictors like Income that don't fall into this category. In Model 3, including all the variables is a pretty straight-forward option to pick, but we run the risk of over-fitting the model and losing quality in our predictions as a result.

Let's fit and compare:



```{r}
#Establish each model recipe
#Model 1: 3 predictors
log_rec1 <- recipe(Diabetes_binary ~ HighChol + BMI + Income,
                   data = diabetes_train) |>
  step_normalize(BMI) |>
  step_dummy(HighChol, Income)

#Model 2: 11 predictors
log_rec2 <- recipe(Diabetes_binary ~ HighBP + HighChol + CholCheck + BMI +
                     Stroke + HeartDiseaseorAttack + PhysActivity +
                     GenHlth + MentHlth + PhysHlth + DiffWalk,
                   data = diabetes_train) |>
  step_normalize(BMI, MentHlth, PhysHlth) |>
  step_dummy(HighBP, HighChol, CholCheck, Stroke, HeartDiseaseorAttack,
               PhysActivity, GenHlth, DiffWalk)

#Model 3: All predictors
log_rec3 <- recipe(Diabetes_binary ~ HighBP + HighChol + CholCheck + BMI +
                     Smoker + Stroke + HeartDiseaseorAttack + PhysActivity +
                     Fruits + Veggies + HvyAlcoholConsump + AnyHealthcare +
                     NoDocbcCost + GenHlth + MentHlth + PhysHlth +
                     DiffWalk + Sex + Age + Education + Income,
                   data = diabetes_train) |>
  step_normalize(BMI, MentHlth, PhysHlth) |>
  step_dummy(HighBP, HighChol, CholCheck, Smoker, Stroke,
             HeartDiseaseorAttack, PhysActivity, Fruits, Veggies,
             HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, GenHlth,
             DiffWalk, Sex, Age, Education, Income)

#Setup glm engine
log_spec <- logistic_reg() |>
  set_engine("glm")

#Setup workflows
log_work1 <- workflow() |>
  add_recipe(log_rec1) |>
  add_model(log_spec)

log_work2 <- workflow() |>
  add_recipe(log_rec2) |>
  add_model(log_spec)

log_work3 <- workflow() |>
  add_recipe(log_rec3) |>
  add_model(log_spec)

#Fit 5-fold CV to each model
log_fit1 <- log_work1 |>
  fit_resamples(diabetes_folds, metrics = metric_set(accuracy, mn_log_loss))

log_fit2 <- log_work2 |>
  fit_resamples(diabetes_folds, metrics = metric_set(accuracy, mn_log_loss))

log_fit3 <- log_work3 |>
  fit_resamples(diabetes_folds, metrics = metric_set(accuracy, mn_log_loss))

#Observe which models performed the best

rbind(log_fit1 |> collect_metrics(),
      log_fit2 |> collect_metrics(),
      log_fit3 |> collect_metrics()) |>
  mutate(Model = c("Model 1", "Model 1", 
                   "Model 2", "Model 2", 
                   "Model 3", "Model 3")) |>
  select(Model, everything())
```



The accuracy for each model is about 86%, 86.5%, and 86.6% respectively. All of our models got similar results! As such, the best of the three models will be the one that minimizes log-loss. In this case, Model 3 minimizes log-loss as it has a value that is about 0.01 less than the next best model (Model 2). Therefore, Model 3 will be our selection from the logistic regression candidates. Let's refit it on the entire training set model now:



```{r}
#Select best fitting parameter
log_final_fit <- log_work3 |>
  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))
```



## Classification Tree

For a classification tree, we are going to use all the predictors by default, so selecting the best classification tree comes down to the one with the best tuning parameter. The tuning parameter in a classification tree attempts to adjust the model for complexity, so the best complexity parameter will be the one that minimizes log-loss for the model.

Let's set a grid of tuning parameters and compare:



```{r}
#Setup classification tree
#Recipe
tree_rec <- recipe(Diabetes_binary ~ ., data = diabetes_train) |>
  step_dummy(all_factor(), -all_outcomes()) |>
  step_normalize(all_numeric())

#Spec and engine with default parameters (and tuning for complexity)
tree_spec <- decision_tree(tree_depth = 15,
                           min_n = 1000,
                           cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

#Set workflow
tree_work <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_spec)

#Set 10 levels of tuning parameters
tree_grid <- grid_regular(cost_complexity(), levels = 5)

#Fit with specified tuning parameters
tree_fits <- tree_work |>
  tune_grid(resamples = diabetes_folds,
            grid = tree_grid,
            metrics = metric_set(accuracy, mn_log_loss))

tree_fits |>
  collect_metrics()
```



Once again, we are in the same ballpark with accuracy at about 86% for every complexity level. Let's select the model with the best parameters and re-fit the training set data to that model.



```{r}
#Select best fitting parameter
tree_best <- select_best(tree_fits, metric = "mn_log_loss")

#Re-fit on entire training set
tree_final <- tree_work |>
  finalize_workflow(tree_best)

tree_final_fit <- tree_final |>
  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))
```



## Random Forest

The tuning parameter of a random forest model is the number of predictors we use in each iteration of the model. We will still fit to our CV folds for the sake of consistency. Let's find the best tuning for the random forest model:



```{r}
#We will re-use the tree recipe from the classification tree

#Set spec and engine
rf_spec <- rand_forest(mtry = tune()) |>
  set_engine("ranger") |>
  set_mode("classification")

#Set workflow
rf_work <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(rf_spec)

#Set constraints for tuning parameters
rf_grid <- grid_regular(mtry(range = c(1, 10)), levels = 5)

#Set fits
rf_fit <- rf_work |>
  tune_grid(resamples = diabetes_folds,
            grid = rf_grid,
            metrics = metric_set(accuracy, mn_log_loss))

#View fits
rf_fit |>
  collect_metrics()
```



Once again, we will select the best tuning parameter based on log-loss. Based on the output above, it looks like the 5-predictor random forest model ended up being the best fit based on log-loss.



```{r}
#Select best tuning parameter
rf_best <- select_best(rf_fit, metric = "mn_log_loss")

#Re-fit on entire training set
rf_final <- rf_work |>
  finalize_workflow(rf_best)

rf_final_fit <- rf_final |>
  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))
```



## Final Model Selection

Now that we've run each type of model and selected a candidate from each, let's fit each model to our test sets and select the best model for predicting diabetes in an individual.



```{r}
rbind(log_final_fit |> collect_metrics(),
      tree_final_fit |> collect_metrics(),
      rf_final_fit |> collect_metrics()) |>
    mutate(Model = c("Logistic", "Logistic", 
                   "Class. Tree", "Class. Tree", 
                   "Random Forest", "Random Forest"))
```



In terms of accuracy, all models perform very similarly. It appears that, based on log-loss, the logistic regression with all 21 predictors was our best fit!

