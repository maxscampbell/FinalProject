---
title: "Diabetes Health Indicators - Modeling"
author: "Max Campbell"
format: html
editor: visual
---

# Modeling our Data

## Introduction

When looking at data like the diabetes health indicators, the first inclination is most likely to try and predict whether a person has diabetes, given the presence (or absence) of these indicators. In order to accomplish this, there are three major options that come to mind: logistic regressions, classification trees, and random forests.

A logistic regression is particularly useful in scenarios where the variable being modeled upon has two factor levels as the response variable will always output a value between 0 and 1, no matter what the indicators are. As a result, the response essentially functions as a probability value that the response's conditions are met. The way this is accomplished is by fitting the probability that the response's conditions are met (hereafter referred to as a "success") as a function of the log-odds of success.

$$
log(\frac{P(Success|x)}{1 - P(Success|x)}) = \beta_0 + \beta_1 x + ...
$$

We use this function in particular because the log-odds of success adopts a linear relationship with the parameters. The end goal of this model would be to quantify the odds of having diabetes given a set of indicators.

A second way we could approach this problem is via classification. This works because we are essentially trying to predict which group an observation is most likely to fall into (presence of diabetes, in this case), so we don't really care about the exact probability, which is what a logistic regression would seek to accomplish. Classification trees work by splitting the data into different regions based on predictors in a way that minimizes the error rate in our prediction. Then, we can give the tree an observation, and it will make decisions based on what conditions the observation meets (in this example, say that the first condition is that the individual has high cholesterol) to sift it through various "bins" of our data. Eventually, the tree will run out of decisions to make, and it will take the most prevalent response within the training data of the final bin and assign an outcome to that observation. We also have the advantage of accounting for complexity in our tree by assigning a parameter that minimizes the error rate of our predictions.

The final way we will consider in this exercise is a random forest. This is a type of ensemble tree method where we bootstrap a large number of individual classification trees in hopes of getting a more accurate prediction. The random forest method works similarly to a classification tree, with the key difference being that a subset of the predictors present in the data are randomly chosen (the exact number of which can be tuned) and subset from the data to obtain a prediction. By taking a random subset of predictors, we reduce the chances that one really strong predictor overtakes the rest of the model and basically becomes the only predictor that it uses to choose an outcome. From there, we iterate on this process numerous times, selecting a different random subset of predictors on each iteration. Once we have completed the bootstrap, we can then "average" our predictions by fitting our data to each tree within the forest and taking the most prevalent outcome per observation. This technique is advantageous because we now gain access to a measure of variability within our bootstrap iterations, so we can see how often our model is deviating from the most prevalent outcome. This is a luxury that is not afforded by a single classification tree.

Now that the process has been outlined, let's get into the specifics of each model type:

## Logistic Regression

## Classification Tree

## Random Forest

## Final Model Selection
