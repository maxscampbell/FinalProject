[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Diabetes Health Indicators - EDA",
    "section": "",
    "text": "It is vitally important in the medical field to have awareness of specific conditions and the risks that individuals may be predisposed to as they go about their lives. One such condition is diabetes, one of the most widespread chronic diseases in the United States. Diabetes can present many health risks to those who have it, and as such there is a vested interest for many people to try and find ways to predict when an individual has a high-risk of developing and/or having diabetes. While it’s unlikely that we will ever be able to perfectly predict whether someone has diabetes or not, we can certainly give it our best shot.\nThe data we have to work with consists of 253,680 survey responses from the Behavioral Risk Factor Surveillance System (BRFSS) conducted by the Center for Disease Control and Prevention (CDC) in 2015. The survey focuses on potential indicators for diabetes in an individual’s medical history via risk factors such as having high blood pressure, high cholesterol, or having had a stroke/heart attack in their past. There are also demographic indicators present such as an individual’s sex, age, education level, or current income. The goal of this survey is to find relationships between these medical/demographic indicators to determine which factors most clearly indicated a high risk for diabetes.\nTo begin, we will choose a subset of these factors to briefly explore in order to get a sense of what factors may have the strongest effect on someone’s risk for developing diabetes:\n\nDiabetes_binary: this is the variable that will act as the response. It is a two-level factor where 0 represents an individual without diabetes and 1 represents an individual with pre-diabetes or diabetes.\nBMI: studies such as this one from the University of Gothenburg in Sweden suggest that BMI may be one of the strongest indicators for diabetes.\nHighChol: short for High Cholesterol, this factor was also named as an influential factor in the study mentioned previously.\nIncome: an individual’s ability to treat any health conditions that may arise can only be as comprehensive as they can afford it to be. As such, it is worth exploring whether an individual’s income has any sort of relationship with the presence of diabetes as a health condition.\n\nBy exploring these factors individually and in relation to the presence of diabetes in an individual, we can hopefully get a sense of how these predictors have an effect on the presence of diabetes and how much they will affect our models when we reach that portion of this analysis.\n\n\n\n\n# Read in dataset\nraw_data &lt;- read.csv(\".\\\\diabetes.csv\", header = TRUE)\n\n# Convert most of these into factors will sensible levels\ndiabetes &lt;- raw_data |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"Low\", \"High\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"Low\", \"High\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1:5)),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n    Age = factor(Age, levels = c(1:13), labels = c(\n      \"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\",\n      \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\",\n      \"70-74\", \"75-79\", \"80&lt;=\"\n    )),\n    Education = factor(Education, levels = c(1:6), labels = c(\n      \"Only Kindergarten or Less\", \"Grade 1-8\", \"Grade 9 - 11\",\n      \"Grade 12 or GED\", \"College 1-3 years\", \"College 4+ years\"\n    )),\n    Income = factor(Income, levels = c(1:8), labels = c(\n      \"&lt; $10k\", \"&lt; $15k\", \"&lt; $20k\", \"&lt; $25k\",\n      \"&lt; 35k\", \"&lt; $50k\", \"&lt; $75k\", \"More than $75k\"\n    )))\n\n# Check for empty cells in the dataset\npaste(\"Empty cells:\", sum(is.na(diabetes)))\n\n[1] \"Empty cells: 0\"\n\n\n\n\n\nNow that our data is cleaned up, let’s start by taking a look at some summary statistics. First off, let’s observe the presence of diabetes in Americans:\n\nsummary(diabetes$Diabetes_binary)\n\n    No    Yes \n218334  35346 \n\n\nThis shows us that 0.1396878% of the people surveyed had diabetes or pre-diabetes. Let’s compare this to those with high cholesterol:\n\nsummary(diabetes$HighChol)\n\n   Low   High \n146089 107591 \n\n\nThis equates to 0.4241209% of people surveyed, which is a lot more prevalent than diabetes. This isn’t too unexpected since diabetes is a specific condition whereas high cholesterol can be a cause and symptom of many conditions. Now let’s look at where these overlap:\n\n#Produce contingency table\ncon.tab &lt;- table(diabetes$HighChol, diabetes$Diabetes_binary)\ncon.tab\n\n      \n           No    Yes\n  Low  134429  11660\n  High  83905  23686\n\n#Calculate rate of diabetes in high-cholesterol individuals\npaste(\"Rate of diabetes in patients with high cholesterol: \", round(con.tab[2,2] / sum(con.tab[2,1], con.tab[2,2]), 2), \"%\", sep = \"\")\n\n[1] \"Rate of diabetes in patients with high cholesterol: 0.22%\"\n\n\nWe can see that the rate went up by about 10%, which feels like a large climb. We should compare this to another two-level factor, such as whether people eat at least 1 serving of fruits per day.\n\n#Produce contingency table\ncon.tab2 &lt;- table(diabetes$Fruits, diabetes$Diabetes_binary)\ncon.tab2\n\n     \n          No    Yes\n  No   78129  14653\n  Yes 140205  20693\n\n#Calculate rate of diabetes in high-cholesterol individuals\npaste(\"Rate of diabetes in patients who eat at least one serving of fruit per day: \", round(con.tab2[2,2] / sum(con.tab2[2,1], con.tab2[2,2]), 2), \"%\", sep = \"\")\n\n[1] \"Rate of diabetes in patients who eat at least one serving of fruit per day: 0.13%\"\n\n\nWe see here that the rate stayed roughly similar to the overall rate of diabetes. As such, we can see that having a high cholesterol level can be a valuable predictor when it comes time to model with this data.\nLet’s also take a look at income as a predictor. First, a look at how income is distributed in this survey:\n\n#Plot a histogram of income levels\nggplot(data = diabetes, aes(x = Income)) +\n  geom_histogram(stat = \"count\", aes(y = after_stat(count / sum(count)))) + #The after_stat function allows us to graph in terms of %\n  labs(title = \"Income Level\", subtitle = \"CDC, 2015\") +\n  ylab(\"Percentage of People Surveyed\")\n\n\n\n\n\n\n\n\nThere’s a very clear left skew in this data. Approximately 40% of people surveyed were making more than $75,000/year in 2015. This means that, if the rate of people getting diabetes was the same across income levels, we should see that most of them are from that top income bracket just because it comprises such a large amount of our observations. However, if income levels do have an effect, we might expect to see deviations from the overall rate, especially at lower income brackets. Let’s take a look now:\n\n#Contingency Table of Income vs Diabetes\ncon.tab3 &lt;- table(diabetes$Income, diabetes$Diabetes_binary)\npercent &lt;- con.tab3[,2] / (con.tab3[,1] + con.tab3[,2])\ntab &lt;- cbind(con.tab3, percent)\ntab\n\n                  No  Yes    percent\n&lt; $10k          7428 2383 0.24289063\n&lt; $15k          8697 3086 0.26190274\n&lt; $20k         12426 3568 0.22308366\n&lt; $25k         16081 4054 0.20134095\n&lt; 35k          21379 4504 0.17401383\n&lt; $50k         31179 5291 0.14507815\n&lt; $75k         37954 5265 0.12182142\nMore than $75k 83190 7195 0.07960392\n\n\nThis is a significant departure from the raw income data! The percent column shown above is the rate of diabetes within each income level. At first, this looks to be expected because the group with the highest raw number of people with diabetes is the highest income bracket. However, when we account for rates of diabetes instead of a simple count, we see a clearer picture. Keeping in mind that the overall rate is about 13%, we see that rates are higher at lower income levels, while they are significantly lower at the highest income level. Likely reasons for this include a lack of access to healthcare and resources to stay healthy.\nNow let’s look at BMI. As an indicator of obesity and general well-being, it would be logical to assume that it has an outsized effect on the rate of diabetes prevalence in the United States. First, let’s take a look at our BMI data by itself:\n\nggplot(data = diabetes, aes(x = BMI)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of BMI\")\n\n\n\n\n\n\n\nsummary(diabetes$BMI)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   24.00   27.00   28.38   31.00   98.00 \n\n\nWe see a wide range of BMIs in our data, with a mean around 28.38 and an IQR of 7. Now let’s see what happens if we split our data up into those with diabetes (or pre-diabetes) and those without.\n\nggplot(data = diabetes, aes(x = BMI, y = Diabetes_binary)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of BMI grouped by Diabetes status\") +\n  ylab(\"Has Diabetes\")\n\n\n\n\n\n\n\n\nOnce again, we see similar shapes with a lot of potential outliers to the higher extremes. However, we do see that the mean BMI notably shifts up to approximately 30-32, which does indicate that having a higher BMI could be an indicator for higher rates of diabetes.\nWith this information, we’ve confirmed our prior inclinations that high cholesterol, higher BMIs, and lower income could have a notable effect on our modeling for this diabetes data, as each indicator shows a significant change in diabetes rates compared to the overall rate when grouped up by diagnosis.\n\n\n\nClick here to see the Modeling portion of this analysis.\n\n\n\nDiabetes Health Indicators Dataset by Alex Teboul\nIdentifying top ten predictors of type 2 diabetes through machine learning analysis of UK Biobank data"
  },
  {
    "objectID": "EDA.html#quarto",
    "href": "EDA.html#quarto",
    "title": "Diabetes Health Indicators - EDA",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "EDA.html#running-code",
    "href": "EDA.html#running-code",
    "title": "Diabetes Health Indicators - EDA",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Diabetes Health Indicators - Modeling",
    "section": "",
    "text": "When looking at data like the diabetes health indicators, the first inclination is most likely to try and predict whether a person has diabetes, given the presence (or absence) of these indicators. In order to accomplish this, there are three major options that come to mind: logistic regressions, classification trees, and random forests.\nA logistic regression is particularly useful in scenarios where the variable being modeled upon has two factor levels as the response variable will always output a value between 0 and 1, no matter what the indicators are. As a result, the response essentially functions as a probability value that the response’s conditions are met. The way this is accomplished is by fitting the probability that the response’s conditions are met (hereafter referred to as a “success”) as a function of the log-odds of success.\n\\[\nlog(\\frac{P(Success|x)}{1 - P(Success|x)}) = \\beta_0 + \\beta_1 x + ...\n\\]\nWe use this function in particular because the log-odds of success adopts a linear relationship with the parameters. The end goal of this model would be to quantify the odds of having diabetes given a set of indicators.\nA second way we could approach this problem is via classification. This works because we are essentially trying to predict which group an observation is most likely to fall into (presence of diabetes, in this case), so we don’t really care about the exact probability, which is what a logistic regression would seek to accomplish. Classification trees work by splitting the data into different regions based on predictors in a way that minimizes the error rate in our prediction. Then, we can give the tree an observation, and it will make decisions based on what conditions the observation meets (in this example, say that the first condition is that the individual has high cholesterol) to sift it through various “bins” of our data. Eventually, the tree will run out of decisions to make, and it will take the most prevalent response within the training data of the final bin and assign an outcome to that observation. We also have the advantage of accounting for complexity in our tree by assigning a parameter that minimizes the error rate of our predictions.\nThe final way we will consider in this exercise is a random forest. This is a type of ensemble tree method where we bootstrap a large number of individual classification trees in hopes of getting a more accurate prediction. The random forest method works similarly to a classification tree, with the key difference being that a subset of the predictors present in the data are randomly chosen (the exact number of which can be tuned) and subset from the data to obtain a prediction. By taking a random subset of predictors, we reduce the chances that one really strong predictor overtakes the rest of the model and basically becomes the only predictor that it uses to choose an outcome. From there, we iterate on this process numerous times, selecting a different random subset of predictors on each iteration. Once we have completed the bootstrap, we can then “average” our predictions by fitting our data to each tree within the forest and taking the most prevalent outcome per observation. This technique is advantageous because we now gain access to a measure of variability within our bootstrap iterations, so we can see how often our model is deviating from the most prevalent outcome. This is a luxury that is not afforded by a single classification tree.\n\n#Set a random seed for reproducible results\nset.seed(72625)\n# Split the dataset into a training set and a test set\ndiabetes_split &lt;- initial_split(diabetes, prop = 0.7)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\ndiabetes_folds &lt;- vfold_cv(diabetes_train, 5)\n\nWhat we just did in the code shown above is split up our initial data into a training split and test split (70%/30%, respectively), and then further split the training set into five cross-validation folds. This will allow us to fit models in a way that allows us to see how much variability our predictions have, which can further optimize our model. We can then take each model and fit on the test set as a way of comparing our models!\nNow that the process has been outlined, let’s get into the specifics of each model type:\n\n\n\nIn our efforts to fit a logistic regression model, we will examine three separate models to fit on the Diabetes_binary variable:\n\nModel 1: the three predictors discussed in the EDA section (high cholesterol, BMI, and income level)\nModel 2: every predictor explicitly related to individual health (high blood pressure, high cholesterol, BMI, history of strokes and/or heart attacks, etc., 11 total).\nModel 3: every predictor provided in the model (21 total).\n\nWe’ve already established that the predictors in Model 1 are likely to significantly drive the model, at the risk of not fitting enough variables to the model. In Model 2, we are trying to see if the health indicators offer a holistic prediction, at the risk of losing valuable information from predictors like Income that don’t fall into this category. In Model 3, including all the variables is a pretty straight-forward option to pick, but we run the risk of over-fitting the model and losing quality in our predictions as a result.\nLet’s fit and compare:\n\n#Establish each model recipe\n#Model 1: 3 predictors\nlog_rec1 &lt;- recipe(Diabetes_binary ~ HighChol + BMI + Income,\n                   data = diabetes_train) |&gt;\n  step_normalize(BMI) |&gt;\n  step_dummy(HighChol, Income)\n\n#Model 2: 11 predictors\nlog_rec2 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + CholCheck + BMI +\n                     Stroke + HeartDiseaseorAttack + PhysActivity +\n                     GenHlth + MentHlth + PhysHlth + DiffWalk,\n                   data = diabetes_train) |&gt;\n  step_normalize(BMI, MentHlth, PhysHlth) |&gt;\n  step_dummy(HighBP, HighChol, CholCheck, Stroke, HeartDiseaseorAttack,\n               PhysActivity, GenHlth, DiffWalk)\n\n#Model 3: All predictors\nlog_rec3 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + CholCheck + BMI +\n                     Smoker + Stroke + HeartDiseaseorAttack + PhysActivity +\n                     Fruits + Veggies + HvyAlcoholConsump + AnyHealthcare +\n                     NoDocbcCost + GenHlth + MentHlth + PhysHlth +\n                     DiffWalk + Sex + Age + Education + Income,\n                   data = diabetes_train) |&gt;\n  step_normalize(BMI, MentHlth, PhysHlth) |&gt;\n  step_dummy(HighBP, HighChol, CholCheck, Smoker, Stroke,\n             HeartDiseaseorAttack, PhysActivity, Fruits, Veggies,\n             HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, GenHlth,\n             DiffWalk, Sex, Age, Education, Income)\n\n#Setup glm engine\nlog_spec &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n#Setup workflows\nlog_work1 &lt;- workflow() |&gt;\n  add_recipe(log_rec1) |&gt;\n  add_model(log_spec)\n\nlog_work2 &lt;- workflow() |&gt;\n  add_recipe(log_rec2) |&gt;\n  add_model(log_spec)\n\nlog_work3 &lt;- workflow() |&gt;\n  add_recipe(log_rec3) |&gt;\n  add_model(log_spec)\n\n#Fit 5-fold CV to each model\nlog_fit1 &lt;- log_work1 |&gt;\n  fit_resamples(diabetes_folds, metrics = metric_set(accuracy, mn_log_loss))\n\nlog_fit2 &lt;- log_work2 |&gt;\n  fit_resamples(diabetes_folds, metrics = metric_set(accuracy, mn_log_loss))\n\nlog_fit3 &lt;- log_work3 |&gt;\n  fit_resamples(diabetes_folds, metrics = metric_set(accuracy, mn_log_loss))\n\n#Observe which models performed the best\n\nrbind(log_fit1 |&gt; collect_metrics(),\n      log_fit2 |&gt; collect_metrics(),\n      log_fit3 |&gt; collect_metrics()) |&gt;\n  mutate(Model = c(\"Model 1\", \"Model 1\", \n                   \"Model 2\", \"Model 2\", \n                   \"Model 3\", \"Model 3\")) |&gt;\n  select(Model, everything())\n\n# A tibble: 6 × 7\n  Model   .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 Model 1 accuracy    binary     0.860     5 0.000508 Preprocessor1_Model1\n2 Model 1 mn_log_loss binary     0.357     5 0.00118  Preprocessor1_Model1\n3 Model 2 accuracy    binary     0.865     5 0.000431 Preprocessor1_Model1\n4 Model 2 mn_log_loss binary     0.324     5 0.000959 Preprocessor1_Model1\n5 Model 3 accuracy    binary     0.866     5 0.000572 Preprocessor1_Model1\n6 Model 3 mn_log_loss binary     0.316     5 0.000856 Preprocessor1_Model1\n\n\nThe accuracy for each model is about 86%, 86.5%, and 86.6% respectively. It’s good that all of our models got similar results! As such, the best of the three models will be the one that normalizes log-loss. In this case, Model 3 minimizes log-loss as it has a value that is about 0.01 less than the next best model (Model 2). Therefore, Model 3 will be our selection from the logistic regression candidates."
  },
  {
    "objectID": "Modeling.html#quarto",
    "href": "Modeling.html#quarto",
    "title": "Diabetes Health Indicators - Modeling",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "Modeling.html#running-code",
    "href": "Modeling.html#running-code",
    "title": "Diabetes Health Indicators - Modeling",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "Diabetes Health Indicators - EDA",
    "section": "",
    "text": "It is vitally important in the medical field to have awareness of specific conditions and the risks that individuals may be predisposed to as they go about their lives. One such condition is diabetes, one of the most widespread chronic diseases in the United States. Diabetes can present many health risks to those who have it, and as such there is a vested interest for many people to try and find ways to predict when an individual has a high-risk of developing and/or having diabetes. While it’s unlikely that we will ever be able to perfectly predict whether someone has diabetes or not, we can certainly give it our best shot.\nThe data we have to work with consists of 253,680 survey responses from the Behavioral Risk Factor Surveillance System (BRFSS) conducted by the Center for Disease Control and Prevention (CDC) in 2015. The survey focuses on potential indicators for diabetes in an individual’s medical history via risk factors such as having high blood pressure, high cholesterol, or having had a stroke/heart attack in their past. There are also demographic indicators present such as an individual’s sex, age, education level, or current income. The goal of this survey is to find relationships between these medical/demographic indicators to determine which factors most clearly indicated a high risk for diabetes.\nTo begin, we will choose a subset of these factors to briefly explore in order to get a sense of what factors may have the strongest effect on someone’s risk for developing diabetes:\n\nDiabetes_binary: this is the variable that will act as the response. It is a two-level factor where 0 represents an individual without diabetes and 1 represents an individual with pre-diabetes or diabetes.\nBMI: studies such as this one from the University of Gothenburg in Sweden suggest that BMI may be one of the strongest indicators for diabetes.\nHighChol: short for High Cholesterol, this factor was also named as an influential factor in the study mentioned previously.\nIncome: an individual’s ability to treat any health conditions that may arise can only be as comprehensive as they can afford it to be. As such, it is worth exploring whether an individual’s income has any sort of relationship with the presence of diabetes as a health condition.\n\nBy exploring these factors individually and in relation to the presence of diabetes in an individual, we can hopefully get a sense of how these predictors have an effect on the presence of diabetes and how much they will affect our models when we reach that portion of this analysis."
  },
  {
    "objectID": "EDA.html#modeling",
    "href": "EDA.html#modeling",
    "title": "Diabetes Health Indicators - EDA",
    "section": "",
    "text": "Click here to see the Modeling portion of this analysis."
  },
  {
    "objectID": "EDA.html#sources",
    "href": "EDA.html#sources",
    "title": "Diabetes Health Indicators - EDA",
    "section": "",
    "text": "Diabetes Health Indicators Dataset by Alex Teboul\nIdentifying top ten predictors of type 2 diabetes through machine learning analysis of UK Biobank data"
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "Diabetes Health Indicators - EDA",
    "section": "",
    "text": "# Read in dataset\nraw_data &lt;- read.csv(\".\\\\diabetes.csv\", header = TRUE)\n\n# Convert most of these into factors will sensible levels\ndiabetes &lt;- raw_data |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"Low\", \"High\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"Low\", \"High\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1:5)),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n    Age = factor(Age, levels = c(1:13), labels = c(\n      \"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\",\n      \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\",\n      \"70-74\", \"75-79\", \"80&lt;=\"\n    )),\n    Education = factor(Education, levels = c(1:6), labels = c(\n      \"Only Kindergarten or Less\", \"Grade 1-8\", \"Grade 9 - 11\",\n      \"Grade 12 or GED\", \"College 1-3 years\", \"College 4+ years\"\n    )),\n    Income = factor(Income, levels = c(1:8), labels = c(\n      \"&lt; $10k\", \"&lt; $15k\", \"&lt; $20k\", \"&lt; $25k\",\n      \"&lt; 35k\", \"&lt; $50k\", \"&lt; $75k\", \"More than $75k\"\n    )))\n\n# Check for empty cells in the dataset\npaste(\"Empty cells:\", sum(is.na(diabetes)))\n\n[1] \"Empty cells: 0\""
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "Diabetes Health Indicators - EDA",
    "section": "",
    "text": "Now that our data is cleaned up, let’s start by taking a look at some summary statistics. First off, let’s observe the presence of diabetes in Americans:\n\nsummary(diabetes$Diabetes_binary)\n\n    No    Yes \n218334  35346 \n\n\nThis shows us that 0.1396878% of the people surveyed had diabetes or pre-diabetes. Let’s compare this to those with high cholesterol:\n\nsummary(diabetes$HighChol)\n\n   Low   High \n146089 107591 \n\n\nThis equates to 0.4241209% of people surveyed, which is a lot more prevalent than diabetes. This isn’t too unexpected since diabetes is a specific condition whereas high cholesterol can be a cause and symptom of many conditions. Now let’s look at where these overlap:\n\n#Produce contingency table\ncon.tab &lt;- table(diabetes$HighChol, diabetes$Diabetes_binary)\ncon.tab\n\n      \n           No    Yes\n  Low  134429  11660\n  High  83905  23686\n\n#Calculate rate of diabetes in high-cholesterol individuals\npaste(\"Rate of diabetes in patients with high cholesterol: \", round(con.tab[2,2] / sum(con.tab[2,1], con.tab[2,2]), 2), \"%\", sep = \"\")\n\n[1] \"Rate of diabetes in patients with high cholesterol: 0.22%\"\n\n\nWe can see that the rate went up by about 10%, which feels like a large climb. We should compare this to another two-level factor, such as whether people eat at least 1 serving of fruits per day.\n\n#Produce contingency table\ncon.tab2 &lt;- table(diabetes$Fruits, diabetes$Diabetes_binary)\ncon.tab2\n\n     \n          No    Yes\n  No   78129  14653\n  Yes 140205  20693\n\n#Calculate rate of diabetes in high-cholesterol individuals\npaste(\"Rate of diabetes in patients who eat at least one serving of fruit per day: \", round(con.tab2[2,2] / sum(con.tab2[2,1], con.tab2[2,2]), 2), \"%\", sep = \"\")\n\n[1] \"Rate of diabetes in patients who eat at least one serving of fruit per day: 0.13%\"\n\n\nWe see here that the rate stayed roughly similar to the overall rate of diabetes. As such, we can see that having a high cholesterol level can be a valuable predictor when it comes time to model with this data.\nLet’s also take a look at income as a predictor. First, a look at how income is distributed in this survey:\n\n#Plot a histogram of income levels\nggplot(data = diabetes, aes(x = Income)) +\n  geom_histogram(stat = \"count\", aes(y = after_stat(count / sum(count)))) + #The after_stat function allows us to graph in terms of %\n  labs(title = \"Income Level\", subtitle = \"CDC, 2015\") +\n  ylab(\"Percentage of People Surveyed\")\n\n\n\n\n\n\n\n\nThere’s a very clear left skew in this data. Approximately 40% of people surveyed were making more than $75,000/year in 2015. This means that, if the rate of people getting diabetes was the same across income levels, we should see that most of them are from that top income bracket just because it comprises such a large amount of our observations. However, if income levels do have an effect, we might expect to see deviations from the overall rate, especially at lower income brackets. Let’s take a look now:\n\n#Contingency Table of Income vs Diabetes\ncon.tab3 &lt;- table(diabetes$Income, diabetes$Diabetes_binary)\npercent &lt;- con.tab3[,2] / (con.tab3[,1] + con.tab3[,2])\ntab &lt;- cbind(con.tab3, percent)\ntab\n\n                  No  Yes    percent\n&lt; $10k          7428 2383 0.24289063\n&lt; $15k          8697 3086 0.26190274\n&lt; $20k         12426 3568 0.22308366\n&lt; $25k         16081 4054 0.20134095\n&lt; 35k          21379 4504 0.17401383\n&lt; $50k         31179 5291 0.14507815\n&lt; $75k         37954 5265 0.12182142\nMore than $75k 83190 7195 0.07960392\n\n\nThis is a significant departure from the raw income data! The percent column shown above is the rate of diabetes within each income level. At first, this looks to be expected because the group with the highest raw number of people with diabetes is the highest income bracket. However, when we account for rates of diabetes instead of a simple count, we see a clearer picture. Keeping in mind that the overall rate is about 13%, we see that rates are higher at lower income levels, while they are significantly lower at the highest income level. Likely reasons for this include a lack of access to healthcare and resources to stay healthy.\nNow let’s look at BMI. As an indicator of obesity and general well-being, it would be logical to assume that it has an outsized effect on the rate of diabetes prevalence in the United States. First, let’s take a look at our BMI data by itself:\n\nggplot(data = diabetes, aes(x = BMI)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of BMI\")\n\n\n\n\n\n\n\nsummary(diabetes$BMI)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   24.00   27.00   28.38   31.00   98.00 \n\n\nWe see a wide range of BMIs in our data, with a mean around 28.38 and an IQR of 7. Now let’s see what happens if we split our data up into those with diabetes (or pre-diabetes) and those without.\n\nggplot(data = diabetes, aes(x = BMI, y = Diabetes_binary)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of BMI grouped by Diabetes status\") +\n  ylab(\"Has Diabetes\")\n\n\n\n\n\n\n\n\nOnce again, we see similar shapes with a lot of potential outliers to the higher extremes. However, we do see that the mean BMI notably shifts up to approximately 30-32, which does indicate that having a higher BMI could be an indicator for higher rates of diabetes.\nWith this information, we’ve confirmed our prior inclinations that high cholesterol, higher BMIs, and lower income could have a notable effect on our modeling for this diabetes data, as each indicator shows a significant change in diabetes rates compared to the overall rate when grouped up by diagnosis."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Diabetes Health Indicators - Modeling",
    "section": "",
    "text": "When looking at data like the diabetes health indicators, the first inclination is most likely to try and predict whether a person has diabetes, given the presence (or absence) of these indicators. In order to accomplish this, there are three major options that come to mind: logistic regressions, classification trees, and random forests.\nA logistic regression is particularly useful in scenarios where the variable being modeled upon has two factor levels as the response variable will always output a value between 0 and 1, no matter what the indicators are. As a result, the response essentially functions as a probability value that the response’s conditions are met. The way this is accomplished is by fitting the probability that the response’s conditions are met (hereafter referred to as a “success”) as a function of the log-odds of success.\n\\[\nlog(\\frac{P(Success|x)}{1 - P(Success|x)}) = \\beta_0 + \\beta_1 x + ...\n\\]\nWe use this function in particular because the log-odds of success adopts a linear relationship with the parameters. The end goal of this model would be to quantify the odds of having diabetes given a set of indicators.\nA second way we could approach this problem is via classification. This works because we are essentially trying to predict which group an observation is most likely to fall into (presence of diabetes, in this case), so we don’t really care about the exact probability, which is what a logistic regression would seek to accomplish. Classification trees work by splitting the data into different regions based on predictors in a way that minimizes the error rate in our prediction. Then, we can give the tree an observation, and it will make decisions based on what conditions the observation meets (in this example, say that the first condition is that the individual has high cholesterol) to sift it through various “bins” of our data. Eventually, the tree will run out of decisions to make, and it will take the most prevalent response within the training data of the final bin and assign an outcome to that observation. We also have the advantage of accounting for complexity in our tree by assigning a parameter that minimizes the error rate of our predictions.\nThe final way we will consider in this exercise is a random forest. This is a type of ensemble tree method where we bootstrap a large number of individual classification trees in hopes of getting a more accurate prediction. The random forest method works similarly to a classification tree, with the key difference being that a subset of the predictors present in the data are randomly chosen (the exact number of which can be tuned) and subset from the data to obtain a prediction. By taking a random subset of predictors, we reduce the chances that one really strong predictor overtakes the rest of the model and basically becomes the only predictor that it uses to choose an outcome. From there, we iterate on this process numerous times, selecting a different random subset of predictors on each iteration. Once we have completed the bootstrap, we can then “average” our predictions by fitting our data to each tree within the forest and taking the most prevalent outcome per observation. This technique is advantageous because we now gain access to a measure of variability within our bootstrap iterations, so we can see how often our model is deviating from the most prevalent outcome. This is a luxury that is not afforded by a single classification tree.\n\n#Set a random seed for reproducible results\nset.seed(72625)\n# Split the dataset into a training set and a test set\ndiabetes_split &lt;- initial_split(diabetes, prop = 0.7)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\ndiabetes_folds &lt;- vfold_cv(diabetes_train, 5)\n\nWhat we just did in the code shown above is split up our initial data into a training split and test split (70%/30%, respectively), and then further split the training set into five cross-validation folds. This will allow us to fit models in a way that allows us to see how much variability our predictions have, which can further optimize our model. We can then take each model and fit on the test set as a way of comparing our models!\nNow that the process has been outlined, let’s get into the specifics of each model type:"
  },
  {
    "objectID": "Modeling.html#logistic-regression",
    "href": "Modeling.html#logistic-regression",
    "title": "Diabetes Health Indicators - Modeling",
    "section": "",
    "text": "In our efforts to fit a logistic regression model, we will examine three separate models to fit on the Diabetes_binary variable:\n\nModel 1: the three predictors discussed in the EDA section (high cholesterol, BMI, and income level)\nModel 2: every predictor explicitly related to individual health (high blood pressure, high cholesterol, BMI, history of strokes and/or heart attacks, etc., 11 total).\nModel 3: every predictor provided in the model (21 total).\n\nWe’ve already established that the predictors in Model 1 are likely to significantly drive the model, at the risk of not fitting enough variables to the model. In Model 2, we are trying to see if the health indicators offer a holistic prediction, at the risk of losing valuable information from predictors like Income that don’t fall into this category. In Model 3, including all the variables is a pretty straight-forward option to pick, but we run the risk of over-fitting the model and losing quality in our predictions as a result.\nLet’s fit and compare:\n\n#Establish each model recipe\n#Model 1: 3 predictors\nlog_rec1 &lt;- recipe(Diabetes_binary ~ HighChol + BMI + Income,\n                   data = diabetes_train) |&gt;\n  step_normalize(BMI) |&gt;\n  step_dummy(HighChol, Income)\n\n#Model 2: 11 predictors\nlog_rec2 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + CholCheck + BMI +\n                     Stroke + HeartDiseaseorAttack + PhysActivity +\n                     GenHlth + MentHlth + PhysHlth + DiffWalk,\n                   data = diabetes_train) |&gt;\n  step_normalize(BMI, MentHlth, PhysHlth) |&gt;\n  step_dummy(HighBP, HighChol, CholCheck, Stroke, HeartDiseaseorAttack,\n               PhysActivity, GenHlth, DiffWalk)\n\n#Model 3: All predictors\nlog_rec3 &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + CholCheck + BMI +\n                     Smoker + Stroke + HeartDiseaseorAttack + PhysActivity +\n                     Fruits + Veggies + HvyAlcoholConsump + AnyHealthcare +\n                     NoDocbcCost + GenHlth + MentHlth + PhysHlth +\n                     DiffWalk + Sex + Age + Education + Income,\n                   data = diabetes_train) |&gt;\n  step_normalize(BMI, MentHlth, PhysHlth) |&gt;\n  step_dummy(HighBP, HighChol, CholCheck, Smoker, Stroke,\n             HeartDiseaseorAttack, PhysActivity, Fruits, Veggies,\n             HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, GenHlth,\n             DiffWalk, Sex, Age, Education, Income)\n\n#Setup glm engine\nlog_spec &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n#Setup workflows\nlog_work1 &lt;- workflow() |&gt;\n  add_recipe(log_rec1) |&gt;\n  add_model(log_spec)\n\nlog_work2 &lt;- workflow() |&gt;\n  add_recipe(log_rec2) |&gt;\n  add_model(log_spec)\n\nlog_work3 &lt;- workflow() |&gt;\n  add_recipe(log_rec3) |&gt;\n  add_model(log_spec)\n\n#Fit 5-fold CV to each model\nlog_fit1 &lt;- log_work1 |&gt;\n  fit_resamples(diabetes_folds, metrics = metric_set(accuracy, mn_log_loss))\n\nlog_fit2 &lt;- log_work2 |&gt;\n  fit_resamples(diabetes_folds, metrics = metric_set(accuracy, mn_log_loss))\n\nlog_fit3 &lt;- log_work3 |&gt;\n  fit_resamples(diabetes_folds, metrics = metric_set(accuracy, mn_log_loss))\n\n#Observe which models performed the best\n\nrbind(log_fit1 |&gt; collect_metrics(),\n      log_fit2 |&gt; collect_metrics(),\n      log_fit3 |&gt; collect_metrics()) |&gt;\n  mutate(Model = c(\"Model 1\", \"Model 1\", \n                   \"Model 2\", \"Model 2\", \n                   \"Model 3\", \"Model 3\")) |&gt;\n  select(Model, everything())\n\n# A tibble: 6 × 7\n  Model   .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 Model 1 accuracy    binary     0.860     5 0.000508 Preprocessor1_Model1\n2 Model 1 mn_log_loss binary     0.357     5 0.00118  Preprocessor1_Model1\n3 Model 2 accuracy    binary     0.865     5 0.000431 Preprocessor1_Model1\n4 Model 2 mn_log_loss binary     0.324     5 0.000959 Preprocessor1_Model1\n5 Model 3 accuracy    binary     0.866     5 0.000572 Preprocessor1_Model1\n6 Model 3 mn_log_loss binary     0.316     5 0.000856 Preprocessor1_Model1\n\n\nThe accuracy for each model is about 86%, 86.5%, and 86.6% respectively. It’s good that all of our models got similar results! As such, the best of the three models will be the one that normalizes log-loss. In this case, Model 3 minimizes log-loss as it has a value that is about 0.01 less than the next best model (Model 2). Therefore, Model 3 will be our selection from the logistic regression candidates."
  }
]